{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a80c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# 1. Exploratory Data Analysis (EDA) and Preprocessing Strategy\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Goal:** Understand the Kepler exoplanet dataset and decide on preprocessing steps for model training.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Adjust path to import from src (if notebooks are in a subdir)\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"module_path = os.path.abspath(os.path.join('..')) # Adjust if your notebooks folder is elsewhere\\n\",\n",
    "    \"if module_path not in sys.path:\\n\",\n",
    "    \"    sys.path.append(module_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src import config\\n\",\n",
    "    \"from src import data_loader\\n\",\n",
    "    \"\\n\",\n",
    "    \"pd.set_option('display.max_columns', 100)\\n\",\n",
    "    \"pd.set_option('display.max_rows', 100)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.1 Load Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"raw_df = data_loader.load_data(config.RAW_DATA_FILE)\\n\",\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    print(f\\\"Dataset shape: {raw_df.shape}\\\")\\n\",\n",
    "    \"    display(raw_df.head())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.2 Basic Information\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    raw_df.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    # Clean column names for easier access\\n\",\n",
    "    \"    raw_df.columns = raw_df.columns.str.strip()\\n\",\n",
    "    \"    display(raw_df.describe(include='all'))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.3 Target Variable Analysis (`koi_disposition`)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None and config.TARGET_COLUMN in raw_df.columns:\\n\",\n",
    "    \"    print(f\\\"Value counts for target variable '{config.TARGET_COLUMN}':\\\")\\n\",\n",
    "    \"    print(raw_df[config.TARGET_COLUMN].value_counts())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.figure(figsize=(8, 5))\\n\",\n",
    "    \"    sns.countplot(x=config.TARGET_COLUMN, data=raw_df, order=raw_df[config.TARGET_COLUMN].value_counts().index)\\n\",\n",
    "    \"    plt.title(f'Distribution of {config.TARGET_COLUMN}')\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Proposed mapping for binary classification\\n\",\n",
    "    \"    print(f\\\"\\\\nPositive labels (map to 1): {config.POSITIVE_LABELS}\\\")\\n\",\n",
    "    \"    print(f\\\"Negative label (map to 0): {config.NEGATIVE_LABEL}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Target column '{config.TARGET_COLUMN}' not found.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The dataset appears somewhat imbalanced. `FALSE POSITIVE` is the majority class. `CONFIRMED` and `CANDIDATE` will be combined into the positive class (exoplanet).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.4 Missing Values Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    missing_values = raw_df.isnull().sum()\\n\",\n",
    "    \"    missing_percent = (raw_df.isnull().sum() / len(raw_df)) * 100\\n\",\n",
    "    \"    missing_df = pd.DataFrame({'count': missing_values, 'percent': missing_percent})\\n\",\n",
    "    \"    missing_df = missing_df[missing_df['count'] > 0].sort_values(by='percent', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not missing_df.empty:\\n\",\n",
    "    \"        print(\\\"Features with missing values:\\\")\\n\",\n",
    "    \"        display(missing_df)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"        sns.barplot(x=missing_df.index, y='percent', data=missing_df)\\n\",\n",
    "    \"        plt.xticks(rotation=90)\\n\",\n",
    "    \"        plt.title('Percentage of Missing Values by Feature')\\n\",\n",
    "    \"        plt.ylabel('Percentage Missing (%)')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"No missing values found in the dataset.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Many features have missing values. The error columns (`_err1`, `_err2`) seem to have a significant number. Stellar parameters (`koi_steff_err1`, etc.) also have many NaNs. Median imputation for numerical features is a reasonable strategy to start with.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.5 Feature Selection and Dropping Columns\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on `config.py` and initial understanding:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    print(\\\"Columns to be dropped (from config.py):\\\")\\n\",\n",
    "    \"    # Filter to show only columns that actually exist in the DataFrame\\n\",\n",
    "    \"    existing_cols_to_drop = [col for col in config.FEATURES_TO_DROP if col in raw_df.columns]\\n\",\n",
    "    \"    print(existing_cols_to_drop)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # koi_pdisposition vs koi_disposition\\n\",\n",
    "    \"    if 'koi_pdisposition' in raw_df.columns and 'koi_disposition' in raw_df.columns:\\n\",\n",
    "    \"        print(\\\"\\\\nComparison of 'koi_pdisposition' and 'koi_disposition':\\\")\\n\",\n",
    "    \"        print(\\\"koi_pdisposition value counts:\\\")\\n\",\n",
    "    \"        print(raw_df['koi_pdisposition'].value_counts())\\n\",\n",
    "    \"        # This cross-tabulation can show how they relate\\n\",\n",
    "    \"        # display(pd.crosstab(raw_df['koi_disposition'], raw_df['koi_pdisposition']))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # koi_score - often a derived score, good to check its distribution if not dropped\\n\",\n",
    "    \"    if 'koi_score' in raw_df.columns:\\n\",\n",
    "    \"        print(\\\"\\\\n'koi_score' distribution (if not dropped):\\\")\\n\",\n",
    "    \"        #raw_df['koi_score'].hist(bins=50)\\n\",\n",
    "    \"        #plt.title('koi_score distribution')\\n\",\n",
    "    \"        #plt.show()\\n\",\n",
    "    \"        print(\\\"Dropping 'koi_score' as it might be a pre-computed probability or cause leakage.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"`koi_pdisposition` seems redundant or an earlier version of `koi_disposition`. `koi_score` is a strong candidate for removal to avoid data leakage if it's a model-derived score.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The `FEATURES_TO_DROP` list in `config.py` looks reasonable for a first pass.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.6 Numerical Feature Analysis (Post Initial Drops)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    df_temp = raw_df.copy()\\n\",\n",
    "    \"    df_temp.columns = df_temp.columns.str.strip() # Ensure clean names\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Simulate dropping columns as per config (excluding target for now)\\n\",\n",
    "    \"    cols_to_drop_sim = [col for col in config.FEATURES_TO_DROP if col in df_temp.columns and col != config.TARGET_COLUMN]\\n\",\n",
    "    \"    df_features = df_temp.drop(columns=cols_to_drop_sim)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Identify numerical features from the remaining set\\n\",\n",
    "    \"    numerical_features = df_features.select_dtypes(include=np.number).columns.tolist()\\n\",\n",
    "    \"    if config.TARGET_COLUMN in numerical_features: # Should not be, but as a check\\n\",\n",
    "    \"        numerical_features.remove(config.TARGET_COLUMN)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    print(f\\\"Identified {len(numerical_features)} numerical features after initial drops:\\\")\\n\",\n",
    "    \"    # print(numerical_features)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Display histograms for a subset of numerical features\\n\",\n",
    "    \"    if numerical_features:\\n\",\n",
    "    \"        print(\\\"\\\\nPlotting histograms for a sample of numerical features...\\\")\\n\",\n",
    "    \"        sample_num_features = np.random.choice(numerical_features, min(len(numerical_features), 9), replace=False)\\n\",\n",
    "    \"        df_features[sample_num_features].hist(bins=20, figsize=(15, 10), layout=(-1, 3))\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Correlation Heatmap (for numerical features only)\\n\",\n",
    "    \"    if numerical_features and len(numerical_features) > 1:\\n\",\n",
    "    \"        print(\\\"\\\\nPlotting correlation heatmap for numerical features (can be slow if many features)...\\\")\\n\",\n",
    "    \"        # For performance, consider a subset or sampling for the heatmap if too many features\\n\",\n",
    "    \"        # We also need to handle NaNs before .corr()\\n\",\n",
    "    \"        df_corr = df_features[numerical_features].copy()\\n\",\n",
    "    \"        # Simple median imputation for heatmap purposes only\\n\",\n",
    "    \"        for col in df_corr.columns:\\n\",\n",
    "    \"            if df_corr[col].isnull().any():\\n\",\n",
    "    \"                 df_corr[col] = df_corr[col].fillna(df_corr[col].median())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Select a smaller subset for cleaner visualization if many features\\n\",\n",
    "    \"        if len(numerical_features) > 30:\\n\",\n",
    "    \"            print(\\\"Selecting top 30 features by variance for correlation heatmap due to high dimensionality.\\\")\\n\",\n",
    "    \"            # Select features with highest variance for a more manageable heatmap\\n\",\n",
    "    \"            top_variance_features = df_corr.var().nlargest(30).index\\n\",\n",
    "    \"            correlation_matrix = df_corr[top_variance_features].corr()\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            correlation_matrix = df_corr.corr()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        plt.figure(figsize=(18, 15))\\n\",\n",
    "    \"        sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\\n\",\n",
    "    \"        plt.title('Correlation Heatmap of Numerical Features')\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"Not enough numerical features to plot a correlation heatmap.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Many features are highly skewed (e.g., `koi_period`, `koi_duration`). Scaling will be important. Some features might be highly correlated (e.g., a feature and its error terms, or different flux measurements). The Random Forest model can handle correlated features to some extent, but severe multicollinearity might be something to look into for other model types.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.7 Categorical Features (Post Initial Drops)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if raw_df is not None:\\n\",\n",
    "    \"    # Using df_features from previous cell (raw_df after config drops)\\n\",\n",
    "    \"    categorical_features = df_features.select_dtypes(include='object').columns.tolist()\\n\",\n",
    "    \"    if config.TARGET_COLUMN in categorical_features:\\n\",\n",
    "    \"        categorical_features.remove(config.TARGET_COLUMN) # Target is handled separately\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    if categorical_features:\\n\",\n",
    "    \"        print(f\\\"Identified categorical features: {categorical_features}\\\")\\n\",\n",
    "    \"        for cat_col in categorical_features:\\n\",\n",
    "    \"            print(f\\\"\\\\nValue counts for {cat_col}:\\\")\\n\",\n",
    "    \"            print(df_features[cat_col].value_counts(dropna=False))\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"No remaining categorical features (excluding target) after initial drops and type selection.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The Kepler dataset, after dropping IDs and text descriptions like `koi_comment`, primarily contains numerical data or flags that can be treated as numerical (e.g., `koi_fpflag_nt` which is 0 or 1). The `src/preprocessor.py` currently drops any remaining object-type columns. If any important categorical features were identified, they would need encoding (e.g., one-hot encoding).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1.8 Preprocessing Strategy Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on the EDA, the `src/preprocessor.py` strategy is reasonable:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1.  **Load Data**: Using `data_loader.py`.\\n\",\n",
    "    \"2.  **Clean Column Names**: Strip spaces.\\n\",\n",
    "    \"3.  **Target Variable Encoding**: Map `koi_disposition` to binary (1 for `CONFIRMED`/`CANDIDATE`, 0 for `FALSE POSITIVE`). Filter out any other disposition values.\\n\",\n",
    "    \"4.  **Feature Dropping**: Remove columns listed in `config.FEATURES_TO_DROP` (identifiers, redundant info like `koi_pdisposition`, `koi_score`, text comments).\\n\",\n",
    "    \"5.  **Handle Remaining Categorical Features**: Currently, `preprocessor.py` drops any remaining non-numerical features. This is acceptable as most data is numerical/flags.\\n\",\n",
    "    \"6.  **Missing Value Imputation**: Use `SimpleImputer` with `median` strategy for all numerical features. This is robust to outliers.\\n\",\n",
    "    \"7.  **Feature Scaling**: Use `StandardScaler` on numerical features after imputation. This is important for many ML algorithms, though Random Forest is less sensitive to it.\\n\",\n",
    "    \"8.  **Train-Test Split**: Stratified split to maintain class proportions, using parameters from `config.py`.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"This notebook provides the rationale for the steps implemented in `src/preprocessor.py`. The next notebook (`02_model_training_and_evaluation.ipynb`) will use these preprocessed data to train and evaluate models.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
